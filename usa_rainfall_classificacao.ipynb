{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6569cc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, KFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c16438",
   "metadata": {},
   "outputs": [],
   "source": [
    "#log-transform\n",
    "def log_transform(X, columns):\n",
    "    X = X.copy()\n",
    "    for col in columns:\n",
    "        X[col] = np.log(X[col])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38db98ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_column_names(df):\n",
    "    df = df.rename(columns={\n",
    "        'Temperature': 'Temperatura',\n",
    "        'Humidity': 'Umidade',\n",
    "        'Wind Speed': 'Velocidade_do_Vento',\n",
    "        'Cloud Cover': 'Cobertura_de_Nuvens',\n",
    "        'Pressure': 'Pressao',\n",
    "        'Location': 'Localizacao',\n",
    "        'Rain Tomorrow': 'Chovera_Amanha',\n",
    "        'Precipitation': 'Precipitacao',\n",
    "        'Date':'Data',\n",
    "    })\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af68872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_season(date):\n",
    "        month = date.month\n",
    "        day = date.day\n",
    "        if ((month >= 3 and day >= 20) and (month <= 6 and day < 21)):\n",
    "                return 'Primavera'\n",
    "        elif (month >= 6 and ( month <= 9 and day < 23)):\n",
    "                return 'Verão'\n",
    "        elif (month >= 9 and (month <= 12 and day < 21)):\n",
    "                return 'Outono'\n",
    "        else:\n",
    "                return 'Inverno'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afeb6b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "cidade_para_estado = {\n",
    "    'Los Angeles': 'California', \n",
    "    'Chicago': 'Illinois', \n",
    "    'Houston': 'Texas',\n",
    "    'Phoenix': 'Arizona',\n",
    "    'Philadelphia': 'Pennsylvania',\n",
    "    'San Antonio': 'Texas', \n",
    "    'San Diego': 'California',\n",
    "    'Dallas': 'Texas',\n",
    "    'San Jose': 'California', \n",
    "    'Austin': 'Texas', \n",
    "    'Jacksonville': 'Florida', \n",
    "    'Fort Worth': 'Texas',\n",
    "    'Columbus': 'Ohio', \n",
    "    'Indianapolis': 'Indiana', \n",
    "    'Charlotte': 'North Carolina', \n",
    "    'San Francisco': 'California',\n",
    "    'Seattle': 'Washington', \n",
    "    'Denver': 'Colorado', \n",
    "    'New York': 'New York', \n",
    "    'Washington D.C.': \n",
    "    'District of Columbia'\n",
    "}\n",
    "estado_para_regiao = {\n",
    "    'California': 'Oeste',\n",
    "    'Illinois': 'Meio-Oeste',\n",
    "    'Texas': 'Sul',\n",
    "    'Arizona': 'Oeste',\n",
    "    'Pennsylvania': 'Nordeste',\n",
    "    'Florida': 'Sul',\n",
    "    'Ohio': 'Meio-Oeste',\n",
    "    'Indiana': 'Meio-Oeste',\n",
    "    'North Carolina': 'Sul',\n",
    "    'Washington': 'Oeste',\n",
    "    'Colorado': 'Oeste',\n",
    "    'New York': 'Nordeste',\n",
    "    'District of Columbia': 'Sul'\n",
    "}\n",
    "def cidade_para_regiao(cidade: pd.Series) -> str:\n",
    "    cidade = cidade.title()\n",
    "    estado = cidade_para_estado.get(cidade)\n",
    "    return estado_para_regiao.get(estado) if estado else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277b7308",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features_pt = ['Temperatura', 'Umidade', 'Velocidade_do_Vento', 'Cobertura_de_Nuvens', 'Pressao']\n",
    "categorical_features_pt = ['Estacao', 'Regiao']\n",
    "\n",
    "def pre_processar_df(df, numeric_columns):\n",
    "    # Transformando os dados numéricos para o log\n",
    "    df = log_transform(df, numeric_columns)\n",
    "\n",
    "    # Cria a coluna Estacao\n",
    "    df['Data'] = pd.to_datetime(df['Data'])\n",
    "    \n",
    "    df[\"Estacao\"] = df[\"Data\"].apply(get_season)\n",
    "    \n",
    "    # Cria a coluna Regiao\n",
    "    df['Regiao'] = df['Localizacao'].apply(cidade_para_regiao)\n",
    "\n",
    "    # Remove colunas originais que não serão mais usadas\n",
    "    df = df.drop(['Data', 'Localizacao'], axis=1)\n",
    "    return df\n",
    "\n",
    "# Para usar no pipeline:\n",
    "preprocess_func = FunctionTransformer(pre_processar_df, kw_args={'numeric_columns': numeric_features_pt}, validate=False)\n",
    "\n",
    "\n",
    "preprocess_before_transforms_pipeline = Pipeline([\n",
    "    ('preprocess_func', preprocess_func)\n",
    "])\n",
    "\n",
    "numeric_transformer = Pipeline([\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "    ('onehot', OneHotEncoder(sparse_output=False))\n",
    "])\n",
    "\n",
    "# Preprocessamento\n",
    "# Cria o pré-processador\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_transformer, numeric_features_pt),\n",
    "    ('cat', categorical_transformer, categorical_features_pt)\n",
    "], remainder='passthrough')\n",
    "\n",
    "preprocessor_without_scaling = ColumnTransformer([\n",
    "    ('cat', categorical_transformer, categorical_features_pt)\n",
    "], remainder='passthrough')\n",
    "\n",
    "\n",
    "# Criando as pipelines\n",
    "pipeline_sem_scaling = Pipeline([\n",
    "    ('preprocess_func', preprocess_func),\n",
    "    ('preprocessor', preprocessor_without_scaling),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocess_func', preprocess_func),\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "search_spaces_logistic = {\n",
    "    'classifier__C': Real(1e-3, 1.0, prior='log-uniform'),\n",
    "    'classifier__max_iter': Integer(100, 1000),\n",
    "}\n",
    "\n",
    "search_spaces_random_forest = {\n",
    "    'classifier__n_estimators': Integer(50, 100),\n",
    "    'classifier__max_depth': Integer(2, 10),\n",
    "    'classifier__bootstrap': Categorical([True, False])\n",
    "}\n",
    "\n",
    "search_spaces_decision_tree = {\n",
    "    'classifier__criterion': Categorical(['gini', 'entropy']),\n",
    "    'classifier__splitter': Categorical(['best', 'random'])\n",
    "}\n",
    "\n",
    "search_spaces_knn = {\n",
    "    'classifier__n_neighbors': Integer(1, 10),\n",
    "    'classifier__weights': Categorical(['uniform', 'distance']),\n",
    "    'classifier__algorithm': Categorical(['auto', 'ball_tree', 'kd_tree'])\n",
    "}\n",
    "\n",
    "search_spaces_sgd = {\n",
    "    'classifier__loss': Categorical(['hinge', 'log_loss', 'squared_hinge', 'modified_huber']),\n",
    "    'classifier__penalty': Categorical(['l2', 'l1', 'elasticnet']),\n",
    "    'classifier__alpha': Real(1e-5, 1e-1, prior='log-uniform')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f43c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar dados para pipeline e vizualiuzação do durante o notebook\n",
    "df_to_pipeline_use = pd.read_csv('usa_rain_prediction_dataset_2024_2025.csv')  # Substitua com o caminho correto\n",
    "df = df_to_pipeline_use.copy()\n",
    "print(df.head())\n",
    "\n",
    "# Atualiza os nomes das colunas para português para facilitar a leitura\n",
    "# e a visualização dos dados\n",
    "df = update_column_names(df)\n",
    "df_to_pipeline_use = update_column_names(df_to_pipeline_use)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80327de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faz o log das colunas numéricas para evitar problemas de assimetria\n",
    "# e melhorar a performance do modelo\n",
    "log_transform_columns = numeric_features_pt.copy()\n",
    "df = log_transform(df, log_transform_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1690d2b-09a3-4ad2-9edd-e7c624f6784b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atualiza os nomes das colunas para português\n",
    "df['Data'] = pd.to_datetime(df['Data'])\n",
    "\n",
    "df[\"Estacao\"] = df[\"Data\"].apply(get_season)\n",
    "\n",
    "# Cria a coluna Regiao e faz o OneHotEncoding para a coluna Estacao\n",
    "# Não usado durante o pipeline, mas necessário para visualização\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "estacao_encoded = encoder.fit_transform(df[[\"Estacao\"]])\n",
    "estacao_df = pd.DataFrame(estacao_encoded, columns=encoder.get_feature_names_out([\"Estacao\"]))\n",
    "df = pd.concat([df, estacao_df], axis=1)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b4b4d6-4c3c-4542-b347-ae9858ee43d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapeamento completo de cidades para estados\n",
    "cidade_para_estado = {\n",
    "    'Los Angeles': 'California',\n",
    "    'Chicago': 'Illinois',\n",
    "    'Houston': 'Texas',\n",
    "    'Phoenix': 'Arizona',\n",
    "    'Philadelphia': 'Pennsylvania',\n",
    "    'San Antonio': 'Texas',\n",
    "    'San Diego': 'California',\n",
    "    'Dallas': 'Texas',\n",
    "    'San Jose': 'California',\n",
    "    'Austin': 'Texas',\n",
    "    'Jacksonville': 'Florida',\n",
    "    'Fort Worth': 'Texas',\n",
    "    'Columbus': 'Ohio',\n",
    "    'Indianapolis': 'Indiana',\n",
    "    'Charlotte': 'North Carolina',\n",
    "    'San Francisco': 'California',\n",
    "    'Seattle': 'Washington',\n",
    "    'Denver': 'Colorado',\n",
    "    'New York': 'New York',\n",
    "    'Washington D.C.': 'District of Columbia'\n",
    "}\n",
    "\n",
    "estado_para_regiao = {\n",
    "    'California': 'Oeste',\n",
    "    'Illinois': 'Meio-Oeste',\n",
    "    'Texas': 'Sul',\n",
    "    'Arizona': 'Oeste',\n",
    "    'Pennsylvania': 'Nordeste',\n",
    "    'Florida': 'Sul',\n",
    "    'Ohio': 'Meio-Oeste',\n",
    "    'Indiana': 'Meio-Oeste',\n",
    "    'North Carolina': 'Sul',\n",
    "    'Washington': 'Oeste',\n",
    "    'Colorado': 'Oeste',\n",
    "    'New York': 'Nordeste',\n",
    "    'District of Columbia': 'Sul'\n",
    "}\n",
    "\n",
    "# Cria a coluna Regiao para cada cidade e verifica se todas as cidades foram mapeadas\n",
    "df['Regiao'] = df['Localizacao'].apply(cidade_para_regiao)\n",
    "\n",
    "cidades_nao_mapeadas = df[df['Regiao'].isna()]['Localizacao'].unique()\n",
    "if len(cidades_nao_mapeadas) > 0:\n",
    "    print(f\"\\nCidades não mapeadas: {cidades_nao_mapeadas}\")\n",
    "else:\n",
    "    print(\"\\nTodas as cidades foram mapeadas com sucesso!\")\n",
    "\n",
    "# Faz o OneHotEncoding para a coluna Regiao\n",
    "# Não usado durante o pipeline, mas necessário para visualização\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "region_encoded = encoder.fit_transform(df[['Regiao']])\n",
    "region_df = pd.DataFrame(region_encoded, columns=encoder.get_feature_names_out(['Regiao']))\n",
    "df = pd.concat([df, region_df], axis=1)\n",
    "\n",
    "print(f\"Regiao Nan sum: {df['Regiao'].isna().sum()}\")\n",
    "df.head(n=20)\n",
    "\n",
    "# Dropa as colunas que não serão mais usadas\n",
    "df = df.drop(['Data', 'Localizacao', 'Estacao', 'Regiao'], axis=1)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284d3bc7-d66d-482f-a880-dc6789f6b89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Separa os dados em X e y e divide em treino e teste\n",
    "\n",
    "X = df.drop('Chovera_Amanha', axis=1)\n",
    "y = df['Chovera_Amanha']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y, shuffle=True)\n",
    "print(f\"X_train shape: {X.shape}\")\n",
    "print(X_train.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5027c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.var(numeric_only=True))\n",
    "print(df.info())\n",
    "print(df.describe())\n",
    "print(df.isnull().sum())\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(df.corr(numeric_only=True), annot=True, cmap='coolwarm', fmt='.2f', annot_kws={\"size\":10})\n",
    "plt.title('Matriz de Correlação')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8830f748",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avaliar_modelos(X_train, X_test, y_train, y_test, scaled=False):\n",
    "    if scaled:\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "    \n",
    "    modelos = {\n",
    "        'Logistic Regression': LogisticRegression(max_iter=1000, C=1.0),\n",
    "        'Decision Tree': DecisionTreeClassifier(max_depth=5),\n",
    "        'Random Forest': RandomForestClassifier()\n",
    "    }\n",
    "\n",
    "    resultados = []\n",
    "\n",
    "    for nome, modelo in modelos.items():\n",
    "        modelo.fit(X_train, y_train)\n",
    "        y_pred_train = modelo.predict(X_train)\n",
    "        y_pred_test = modelo.predict(X_test)\n",
    "\n",
    "        resultados.append({\n",
    "            'Modelo': nome + (' (Escalado)' if scaled else ''),\n",
    "            'Accuracy Train': accuracy_score(y_train, y_pred_train),\n",
    "            'Accuracy Test': accuracy_score(y_test, y_pred_test),\n",
    "            'Precision Train': precision_score(y_train, y_pred_train),\n",
    "            'Precision Test': precision_score(y_test, y_pred_test),\n",
    "            'Recall Train': recall_score(y_train, y_pred_train),\n",
    "            'Recall Test': recall_score(y_test, y_pred_test),\n",
    "            'F1 Score Train': f1_score(y_train, y_pred_train),\n",
    "            'F1 Score Test': f1_score(y_test, y_pred_test),\n",
    "            'Melhores Hiperparâmetros': None,\n",
    "            'y_pred_train': y_pred_train,\n",
    "            'y_pred_test': y_pred_test,\n",
    "            'Melhor Modelo': None,\n",
    "        })\n",
    "\n",
    "    return resultados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bb4492",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avaliar_modelos_pipeline(X_train, X_test, y_train, y_test, search_spaces, modelo = LogisticRegression, scaled=False):\n",
    "    pipeline_a_ser_usada = pipeline_sem_scaling if not scaled else pipeline\n",
    "    \n",
    "    pipeline_a_ser_usada.set_params(classifier=modelo)\n",
    "    bayes_search = BayesSearchCV(\n",
    "        estimator=pipeline_a_ser_usada,\n",
    "        search_spaces=search_spaces,\n",
    "        n_iter=10,  # Ajuste conforme o tempo disponível\n",
    "        cv=10,\n",
    "        n_jobs=-1,\n",
    "        verbose=2,\n",
    "        random_state=42\n",
    "    )\n",
    "    print(f\"Treinando com a Pipeline: {pipeline_a_ser_usada}\")\n",
    "    # Treina o BayesSearchCV\n",
    "\n",
    "    # Pega o melhor pipeline encontrado\n",
    "    melhor_pipeline = bayes_search.best_estimator_\n",
    "    \n",
    "    # Faz predições com o melhor pipeline\n",
    "    y_pred_train = cross_val_predict(melhor_pipeline, X_train, y_train, cv=10, n_jobs=-1)\n",
    "    \n",
    "    melhor_pipeline.fit(X_train, y_train)\n",
    "    y_pred_test = melhor_pipeline.predict(X_test)\n",
    "\n",
    "    return {\n",
    "        'Modelo': type(melhor_pipeline.named_steps['classifier']).__name__ + (' (Escalado)' if scaled else ''),\n",
    "        'Melhor Modelo' : melhor_pipeline,\n",
    "        'Accuracy Train': accuracy_score(y_train, y_pred_train),\n",
    "        'Accuracy Test': accuracy_score(y_test, y_pred_test),\n",
    "        'Precision Train': precision_score(y_train, y_pred_train),\n",
    "        'Precision Test': precision_score(y_test, y_pred_test),\n",
    "        'Recall Train': recall_score(y_train, y_pred_train),\n",
    "        'Recall Test': recall_score(y_test, y_pred_test),\n",
    "        'F1 Score Train': f1_score(y_train, y_pred_train),\n",
    "        'F1 Score Test': f1_score(y_test, y_pred_test),\n",
    "        'y_pred_train': y_pred_train,\n",
    "        'y_pred_test': y_pred_test,\n",
    "        'Melhores Hiperparâmetros': bayes_search.best_params_\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0817842f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pipeline = df_to_pipeline_use.drop('Chovera_Amanha', axis=1)\n",
    "y_pipeline = df_to_pipeline_use['Chovera_Amanha']\n",
    "X_train_pipe, X_test_pipe, y_train_pipe, y_test_pipe = train_test_split(X_pipeline, y_pipeline, test_size=0.2, random_state=42, stratify=y_pipeline, shuffle=True)\n",
    "\n",
    "# Avaliando os modelos sem pipeline não escalado\n",
    "resultados_sem_pipeline = avaliar_modelos(X_train, X_test, y_train, y_test, scaled=False)\n",
    "resultados_sem_pipeline_scaler = avaliar_modelos(X_train, X_test, y_train, y_test, scaled=True)\n",
    "\n",
    "dict_modelos_pipeline = {\n",
    "    'Logistic Regression': {\n",
    "        'search_spaces': search_spaces_logistic,\n",
    "        'modelo': LogisticRegression()\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'search_spaces': search_spaces_random_forest,\n",
    "        'modelo': RandomForestClassifier()\n",
    "    },\n",
    "    'Decision Tree': {\n",
    "        'search_spaces': search_spaces_decision_tree,\n",
    "        'modelo': DecisionTreeClassifier()\n",
    "    },\n",
    "    'KNN': {\n",
    "        'search_spaces': search_spaces_knn,\n",
    "        'modelo': KNeighborsClassifier()\n",
    "    },\n",
    "    'SGD Classifier': {\n",
    "        'search_spaces': search_spaces_sgd,\n",
    "        'modelo': SGDClassifier()\n",
    "    }\n",
    "}\n",
    "\n",
    "resultados_pipeline = []\n",
    "for info in dict_modelos_pipeline.values():\n",
    "    resultados_pipeline.extend(\n",
    "        (\n",
    "            avaliar_modelos_pipeline(\n",
    "                X_train_pipe,\n",
    "                X_test_pipe,\n",
    "                y_train_pipe,\n",
    "                y_test_pipe,\n",
    "                info['search_spaces'],\n",
    "                info['modelo'],\n",
    "                scaled=False,\n",
    "            ),\n",
    "            avaliar_modelos_pipeline(\n",
    "                X_train_pipe,\n",
    "                X_test_pipe,\n",
    "                y_train_pipe,\n",
    "                y_test_pipe,\n",
    "                info['search_spaces'],\n",
    "                info['modelo'],\n",
    "                scaled=True,\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "    \n",
    "# Avaliando os modelos com pipeline não escalado\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c23d0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "for nome, modelo in {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, C=1.0),\n",
    "    'Decision Tree': DecisionTreeClassifier(max_depth=5),\n",
    "    'Random Forest': RandomForestClassifier()\n",
    "}.items():\n",
    "    scores = cross_val_score(modelo, X, y, cv=kf, scoring='accuracy')\n",
    "    print(f'{nome} - Média Accuracy (K-Fold): {scores.mean():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b53823a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resultados = pd.DataFrame(resultados_sem_pipeline_scaler + resultados_sem_pipeline + resultados_pipeline)\n",
    "df_resultados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49feb208",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker  # Importe para controlar as marcações do eixo\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "df_plot = df_resultados.set_index('Modelo')[[\n",
    "    'Accuracy Train', \n",
    "    'Accuracy Test',\n",
    "    'Precision Train',\n",
    "    'Precision Test',\n",
    "    'Recall Train', \n",
    "    'Recall Test',\n",
    "    'F1 Score Train', \n",
    "    'F1 Score Test']]\n",
    "df_plot.plot(kind='bar', ax=ax)\n",
    "\n",
    "# Configurar marcações do eixo Y (passos de 0.1)\n",
    "ax.yaxis.set_major_locator(ticker.MultipleLocator(0.1))  # Define intervalos de 0.1\n",
    "ax.set_ylim(0, 1)  # Garante que o eixo Y vá de 0 a 1 (ajuste se necessário)\n",
    "\n",
    "# Adicionar grid\n",
    "ax.grid(True, axis='y', linestyle='--', alpha=0.7)  # Grid horizontal tracejado\n",
    "\n",
    "# Ajustar legenda fora do gráfico\n",
    "ax.legend(\n",
    "    bbox_to_anchor=(1.05, 1),\n",
    "    loc='upper left',\n",
    "    borderaxespad=0.1\n",
    ")\n",
    "\n",
    "plt.title('Desempenho dos Modelos (com e sem escalonamento)')\n",
    "plt.ylabel('Pontuação')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
